{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9766af20-7fbc-4f7c-8c61-72e20aa094d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' QUE 1 What is image segmentation, and why is it important?\n",
    "Image segmentation is the process of dividing an image into distinct regions or segments that are easier to analyze. \n",
    "These segments can correspond to different objects, parts of objects, or regions with similar characteristics. \n",
    "The goal is to simplify or change the representation of an image to make it more meaningful and easier to analyze.\n",
    "\n",
    "It is important because it enables more detailed understanding of images. For instance, in medical imaging, segmentation can help identify tumors,\n",
    "in autonomous driving, it can aid in recognizing pedestrians or road signs, and in agriculture, \n",
    "it can help detect diseased crops. Segmentation plays a critical role in tasks that require precise location and boundaries of objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc0ce02-03f2-4d84-b4b1-fddf5ce677c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' QUE 2 Explain the difference between image classification, object detection, and image segmentation\n",
    "\n",
    "Image Classification: The task is to assign a label to the entire image based on its contents (e.g., identifying whether an image is of a dog, \n",
    "a cat, or a car). It's a more general task where the focus is on identifying the main subject of the image.\n",
    "\n",
    "Object Detection: This task involves detecting and locating objects within an image. It outputs both the class label and the bounding \n",
    "box coordinates around each object. For example, it can detect multiple cars in a street scene with their locations.\n",
    "\n",
    "Image Segmentation: This is a more fine-grained approach where the image is divided into segments (or pixels) that correspond to different\n",
    "objects or regions. Instead of just bounding boxes, it assigns a label to each pixel in the image, creating a detailed \"mask\" of the object.\n",
    "It provides a more accurate understanding of the object shapes and boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3d984d-f58d-4e3c-bf66-5c8677b92cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' QUE 3 What is Mask R-CNN, and how is it different from traditional object detection models?\n",
    "\n",
    "Mask R-CNN is an extension of Faster R-CNN, designed to add pixel-level segmentation capability to object detection tasks. \n",
    "While traditional object detection models like Faster R-CNN detect objects by outputting bounding boxes, Mask R-CNN goes further by\n",
    "generating a binary mask for each detected object. This mask allows the model to segment the object more precisely,\n",
    "rather than just providing a box around it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e113b948-6501-484f-93d2-43a224d4f681",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' QUE 4 What role does the \"RoIAlign\" layer play in Mask R-CNN?\n",
    "\n",
    "The RoIAlign layer in Mask R-CNN addresses the problem of misalignment during the region of interest (RoI) pooling stage of Faster\n",
    "R-CNN. RoI pooling, used in traditional object detection models, can distort feature maps by quantizing the coordinates of \n",
    "the regions of interest. RoIAlign, in contrast, performs bilinear interpolation to extract features more accurately from \n",
    "the original feature maps, avoiding the misalignment problem and improving the model’s accuracy \n",
    "for both object detection and segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41afb7a-3d2d-42b2-b479-e77941bc21a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' QUE 5 What are semantic, instance, and panoptic segmentation?\n",
    "\n",
    "Semantic Segmentation: In this approach, every pixel in the image is labeled with a class, but no distinction is made \n",
    "between different instances of the same class. \n",
    "For example, all pixels belonging to \"dogs\" are labeled as \"dog\" without distinguishing between individual dogs.\n",
    "\n",
    "Instance Segmentation: This is a more detailed form of segmentation where each object instance is identified separately. \n",
    "For instance, in an image with three dogs, each dog would be segmented and labeled individually, even if they are of the same class.\n",
    "\n",
    "Panoptic Segmentation: Panoptic segmentation combines the benefits of both semantic and instance segmentation. \n",
    "It assigns each pixel to a class and also distinguishes different instances of those classes. \n",
    "This approach provides a unified view by combining object detection and semantic segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c17d993-947f-426c-9686-8bab67a1d624",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' QUE 6 Describe the role of bounding boxes and masks in image segmentation models\n",
    "\n",
    "In object detection, bounding boxes are used to localize and frame objects of interest in an image. \n",
    "These boxes provide the model with an approximate location of each object, but they do not capture the object’s detailed shape.\n",
    "\n",
    "In instance segmentation, masks provide a pixel-by-pixel segmentation of an object’s boundary. \n",
    "Instead of just identifying the object's location with a bounding box, the mask captures the precise outline of the object. \n",
    "These masks allow for more detailed and accurate object identification, particularly for objects that are irregularly shaped or overlap with others.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782bcf7d-8250-443b-9fa7-544456e1bf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' QUE 7 What is the purpose of data annotation in image segmentation?\n",
    "\n",
    "Data annotation in image segmentation involves labeling images with specific information such as class labels and pixel-wise annotations (masks)\n",
    "for each object in the image. The purpose is to provide a model with accurate ground truth to learn from during training. In segmentation, \n",
    "this means providing detailed masks that represent the boundaries of objects in the image. \n",
    "This annotation allows models to learn how to distinguish and segment objects based on their shape and location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e63a23e-d374-4ba6-a4e9-8c1759cd4890",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' QUE 8 How does Detectron2 simplify model training for object detection and segmentation tasks?\n",
    "\n",
    "Detectron2 is a popular, open-source library developed by Facebook AI Research. It simplifies model training by providing a high-level \n",
    "framework with pre-configured architectures for tasks like object detection, instance segmentation, and panoptic segmentation. \n",
    "Some ways it simplifies training include:\n",
    "\n",
    "Modular Design: Users can easily swap out components, such as backbones, for customized setups.\n",
    "Pre-trained Models: Detectron2 provides pre-trained models, making it easy to fine-tune them on custom datasets using transfer learning.\n",
    "Efficient Training Pipeline: It has optimized implementations of RPNs, RoIAlign, and other components that speed up training.\n",
    "Comprehensive Tools: Includes tools for dataset preparation, visualization, and evaluation, \n",
    "helping streamline the entire process from data to deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da53098-4971-4e56-950e-1e5d2cca4a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' QUE 9 Why is transfer learning valuable in training segmentation models?\n",
    "\n",
    "Transfer learning is valuable because it allows models to leverage knowledge gained from large, pre-trained datasets (like COCO or ImageNet) \n",
    "and apply it to a new task with a smaller custom dataset. This is especially useful in segmentation because annotated segmentation data is \n",
    "often limited, and training models from scratch can be computationally expensive and time-consuming. By fine-tuning pre-trained models on \n",
    "specific tasks, transfer learning helps:\n",
    "\n",
    "Speed up convergence\n",
    "Improve performance even with limited labeled data\n",
    "Make models more robust in recognizing common features that are shared across domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8742a9d7-c8f2-4a0b-9107-386f002c9f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' QUE 10 How does Mask R-CNN improve upon the Faster R-CNN model architecture?\n",
    "\n",
    "Mask R-CNN improves upon Faster R-CNN by adding an additional segmentation head to the architecture. Faster R-CNN performs object detection\n",
    "by outputting bounding boxes and class labels. However, Mask R-CNN extends this capability by producing pixel-wise segmentation masks for \n",
    "each detected object. The key improvements are:\n",
    "\n",
    "Segmentation Head: After detecting the object, Mask R-CNN generates a segmentation mask for each object instance.\n",
    "RoIAlign: To handle the misalignment issue in Faster R-CNN’s RoI pooling, Mask R-CNN uses RoIAlign, which provides more precise feature\n",
    "extraction for segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3fabc3-6370-49f0-9f0a-b4edd68faabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' QUE 11 What is meant by \"from bounding box to polygon masks\" in image segmentation?\n",
    "\n",
    "\"From bounding box to polygon masks\" refers to the transition from the simpler object localization method (bounding boxes) to a more \n",
    "detailed and accurate segmentation approach (polygon masks). While bounding boxes only give the approximate location of an object,\n",
    "polygon masks define the precise shape and boundary of the object at the pixel level, which is important for tasks like instance \n",
    "segmentation where the object boundaries need to be clearly identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759a3e35-5024-4539-92e3-97e438f542f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' QUE 12 How does data augmentation benefit image segmentation model training?\n",
    "\n",
    "Data augmentation involves applying various transformations to the training data (e.g., rotation, flipping, scaling, or color adjustments).\n",
    "For image segmentation models, augmentation helps by:\n",
    "\n",
    "Increasing data diversity: Helps the model generalize better by learning from varied scenarios and transformations.\n",
    "Reducing overfitting: By presenting the model with new variations of the data, it is less likely to memorize specific patterns in the training set.\n",
    "Improving robustness: The model becomes more capable of handling real-world variations, such as changes in lighting, orientation, or size of objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b993e4b-3de2-4691-ab1d-7acac3a9c25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' QUE 13 Describe the architecture of Mask R-CNN, focusing on the backbone, region proposal network (RPN), and segmentation mask head\n",
    "\n",
    "Backbone: The backbone of Mask R-CNN is typically a Convolutional Neural Network (CNN), like ResNet or FPN, which extracts feature maps\n",
    "from the input image. These feature maps capture the hierarchical spatial information necessary for detecting objects.\n",
    "\n",
    "Region Proposal Network (RPN): The RPN generates potential regions (proposals) where objects might be located. It slides a small network\n",
    "over the feature maps produced by the backbone and generates a set of candidate bounding boxes with associated objectness scores.\n",
    "\n",
    "Segmentation Mask Head: After the RPN outputs regions of interest (RoIs), Mask R-CNN uses RoIAlign to precisely extract features for each \n",
    "RoI. Then, it applies a segmentation mask head that outputs a binary mask (of fixed size, typically 28x28 or 56x56) for each detected object,\n",
    "indicating the exact pixels belonging to the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054cddbe-c5a6-4d96-a765-6cf83fe6ce17",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' QUE 14 Explain the process of registering a custom dataset in Detectron2 for model training\n",
    "\n",
    "To register a custom dataset in Detectron2, you typically follow these steps:\n",
    "\n",
    "Prepare Dataset: Organize the dataset into a directory structure, with images and annotations (in formats like COCO or Pascal VOC).\n",
    "Create Dataset Configuration: Write a Python script where you define the dataset's format, such as specifying the paths to the images \n",
    "and annotations, and how to load them.\n",
    "Register Dataset: Use Detectron2’s DatasetCatalog to register your dataset and specify the dataset type (e.g., detection or segmentation).\n",
    "Set up Metadata: Add class names and other metadata for your dataset to help the model understand the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e5b06c-6625-4718-a3e7-96ea673bccd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' QUE 15 What challenges arise in scene understanding for image segmentation, and how can Mask R-CNN address them?\n",
    "\n",
    "Scene understanding in image segmentation comes with several challenges:\n",
    "\n",
    "Occlusion: Objects can overlap, hiding parts of each other, which makes it harder for a model to segment them accurately.\n",
    "Complex Backgrounds: Distracting or cluttered backgrounds can confuse the model, leading to poor segmentation of foreground objects.\n",
    "\n",
    "Object Scale Variability: Objects in an image may vary greatly in size, making it difficult for models to handle both large and small \n",
    "objects simultaneously.\n",
    "\n",
    "Fine-grained Details: Some objects have intricate or detailed boundaries that are hard to segment, especially in natural scenes.\n",
    "Mask R-CNN can address these challenges by:\n",
    "\n",
    "Instance Segmentation: It performs pixel-wise segmentation for individual objects, even when they overlap, which helps with occlusion handling.\n",
    "\n",
    "\n",
    "RoIAlign: The RoIAlign layer improves precision in segmenting objects by extracting better features for each region of interest.\n",
    "Region Proposal Network (RPN): The RPN helps identify potential objects even in cluttered or complex scenes, improving segmentation in challenging\n",
    "backgrounds.\n",
    "\n",
    "Segmentation Masks: By generating accurate masks rather than relying on bounding boxes, Mask R-CNN captures finer object boundaries, helping with \n",
    "fine-grained details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61f014a-d6ac-4515-a0e3-39196c7c011e",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' QUE 16 How is the \"IoU (Intersection over Union)\" metric used in evaluating segmentation models?\n",
    "\n",
    "Intersection over Union (IoU) is a standard metric for evaluating the accuracy of image segmentation models. It measures the overlap\n",
    "between the predicted segmentation mask and the ground truth mask. It is calculated as:\n",
    "\n",
    "IoU = Area of Overlap/Area of Union\n",
    "\n",
    "Where:\n",
    "Area of Overlap is the intersection of the predicted and ground truth masks.\n",
    "\n",
    "Area of Union is the total area covered by both masks combined.\n",
    "\n",
    "The IoU score ranges from 0 (no overlap) to 1 (perfect overlap). Higher IoU values indicate better performance. In the context of segmentation, \n",
    "\n",
    "IoU helps to evaluate how well the model’s predicted boundaries match the true object boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892a58f5-6769-4c28-a879-a846d87002ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' QUE 17 Discuss the use of transfer learning in Mask R-CNN for improving segmentation on custom datasets\n",
    "\n",
    "Transfer learning is particularly useful in Mask R-CNN when you have a small custom dataset but want to leverage the power of \n",
    "pre-trained models on large datasets like COCO. Here's how transfer learning helps:\n",
    "\n",
    "Pre-trained Feature Extractor: Mask R-CNN can start with a backbone (e.g., ResNet) that has already learned useful features from \n",
    "large-scale datasets, such as edges, textures, and patterns. This reduces the need for training the model from scratch and speeds up convergence.\n",
    "Fine-Tuning: You can fine-tune the pre-trained model on your specific custom dataset, allowing it to adjust the features to your data’s unique\n",
    "characteristics while still benefiting from the knowledge learned on the large dataset.\n",
    "\n",
    "Improved Performance: Transfer learning enables the model to perform well even with limited labeled data, as the pre-trained model has already \n",
    "learned useful representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d8614d-49c7-468b-8474-2d1e358a9ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' QUE 18 What is the purpose of evaluation curves, such as precision-recall curves, in segmentation model assessment?\n",
    "\n",
    "Precision-recall curves are used to evaluate segmentation models, especially when dealing with imbalanced datasets where one class might dominate\n",
    "the other. They help to understand how well the model is detecting positive class instances (objects) under different threshold settings:\n",
    "\n",
    "Precision: The proportion of true positive predictions out of all predicted positives. It answers the question: \"Of all the pixels the model labeled\n",
    "as belonging to the object, how many are correct?\"\n",
    "\n",
    "Recall: The proportion of true positive predictions out of all actual positives in the ground truth. It answers the question: \"Of all the actual\n",
    "object pixels, how many did the model detect?\"\n",
    "The precision-recall curve shows the trade-off between precision and recall as you adjust the decision threshold. For imbalanced segmentation tasks,\n",
    "it’s more informative than accuracy, as accuracy may be misleading when one class dominates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a715afd-6a99-4551-9989-98fbb6bad35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' QUE 19 How do Mask R-CNN models handle occlusions or overlapping objects in segmentation?\n",
    "\n",
    "Mask R-CNN handles occlusions or overlapping objects by performing instance segmentation rather than just bounding box detection. \n",
    "This allows it to differentiate between individual objects even when they are partially occluded or overlap:\n",
    "\n",
    "Region Proposal Network (RPN): The RPN generates candidate object proposals, which are then refined and used to produce accurate bounding boxes. \n",
    "Even when objects overlap, Mask R-CNN can still detect the objects by generating separate proposals for each one.\n",
    "Segmentation Masks: Mask R-CNN’s segmentation head provides a pixel-level mask for each object. These masks can represent individual objects even\n",
    "in cases of overlap, allowing the model to segment out objects despite occlusions.\n",
    "The model’s ability to detect and segment individual instances (not just regions) allows it to perform well in challenging scenes with overlapping\n",
    "or occluded objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2902a484-7661-4d17-a105-91c2e31b1e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' QUE 20 Explain the impact of batch size and learning rate on Mask R-CNN model training\n",
    "\n",
    "Batch Size:\n",
    "Larger Batch Size: It allows the model to process more examples at once, which can lead to more stable gradient estimates and faster convergence.\n",
    "However, it also requires more memory.\n",
    "Smaller Batch Size: It uses less memory and allows the model to update weights more frequently, but the gradients may be noisier, and convergence \n",
    "can be slower.\n",
    "Learning Rate:\n",
    "\n",
    "Higher Learning Rate: A higher learning rate can make the model converge faster but might lead to instability or overshooting the optimal solution.\n",
    "It can also cause the model to \"jump over\" narrow minima in the loss landscape.\n",
    "Lower Learning Rate: A lower learning rate typically provides more stable and precise updates, but it might require more training steps and could\n",
    "potentially get stuck in local minima.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdca369-de13-4ec9-9e30-d779382b0589",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' QUE 21 Describe the challenges of training segmentation models on custom datasets, particularly in the context of Detectron2\n",
    "\n",
    "Training segmentation models on custom datasets presents a few challenges:\n",
    "\n",
    "Data Annotation: Labeling data for segmentation can be time-consuming, especially for tasks like instance segmentation, where pixel-level masks \n",
    "are needed for every object in every image.\n",
    "Dataset Size: Custom datasets might be small compared to large benchmark datasets like COCO, leading to overfitting or poor generalization.\n",
    "Class Imbalance: Some object classes might be underrepresented, which can cause the model to be biased toward the more frequent classes.\n",
    "Data Variability: Custom datasets might have variability in terms of object sizes, orientations, lighting conditions, and occlusions, making \n",
    "it hard for the model to generalize well.\n",
    "Detectron2 helps mitigate these challenges by:\n",
    "\n",
    "Offering pre-trained models that can be fine-tuned on custom datasets, reducing the need for extensive training from scratch.\n",
    "Providing tools for dataset management, augmentation, and evaluation, which helps improve model robustness.\n",
    "Allowing easy configuration of hyperparameters, enabling you to experiment and optimize training for your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877a4a4f-d1d1-4c23-945d-be8baa5046b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' QUE 22 How does Mask R-CNN's segmentation head output differ from a traditional object detector’s output?\n",
    "\n",
    "The output of Mask R-CNN's segmentation head is a pixel-wise binary mask for each detected object, while a traditional object detector\n",
    "like Faster R-CNN outputs bounding boxes around the detected objects. The key differences are:\n",
    "\n",
    "Bounding Boxes: Traditional object detectors (like Faster R-CNN) provide coarse localization using rectangular bounding boxes that define\n",
    "the object’s position and extent.\n",
    "Segmentation Masks: Mask R-CNN, in contrast, generates detailed segmentation masks that define the exact shape and boundary of the object, \n",
    "capturing more fine-grained information than just a box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e255a0-06ae-4b52-befd-e557ecec30db",
   "metadata": {},
   "outputs": [],
   "source": [
    "            #PRACTICAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd512bd7-8b05-4859-a502-4fc91f20880a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUE 1  Perform basic color-based segmentation to separate the blue color in an image\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "image = cv2.imread('image.jpg')\n",
    "\n",
    "\n",
    "hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "\n",
    "lower_blue = np.array([100, 150, 50])\n",
    "upper_blue = np.array([140, 255, 255])\n",
    "\n",
    "\n",
    "mask = cv2.inRange(hsv_image, lower_blue, upper_blue)\n",
    "\n",
    "\n",
    "blue_segment = cv2.bitwise_and(image, image, mask=mask)\n",
    "\n",
    "cv2.imshow(\"Blue Segmentation\", blue_segment)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd25c344-a8ba-421f-a9ba-17ed430b60a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUE 2 Use edge detection with Canny to highlight object edges in an image\n",
    "\n",
    "import cv2\n",
    "\n",
    "image = cv2.imread('cat.jpg')\n",
    "\n",
    "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "edges = cv2.Canny(gray_image, 100, 200)\n",
    "\n",
    "cv2.imshow(\"Edges\", edges)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f8e679-5ea3-4ece-9569-f49c5766d2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUE 3 Load a pretrained Mask R-CNN model from PyTorch and use it for object detection and segmentation on an image\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "\n",
    "# Load a pre-trained Mask R-CNN model\n",
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Load and prepare the image\n",
    "image = Image.open(\"image.jpg\")\n",
    "image_tensor = torchvision.transforms.functional.to_tensor(image).unsqueeze(0)\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    predictions = model(image_tensor)\n",
    "\n",
    "# Extract masks and boxes\n",
    "masks = predictions[0]['masks'] > 0.5  # Thresholding the masks\n",
    "boxes = predictions[0]['boxes']\n",
    "\n",
    "# Display results (simple visualization)\n",
    "draw = ImageDraw.Draw(image)\n",
    "for mask, box in zip(masks, boxes):\n",
    "    # Convert mask to numpy array\n",
    "    mask = mask[0].cpu().numpy()\n",
    "    \n",
    "    # Draw bounding box\n",
    "    draw.rectangle(box.tolist(), outline=\"red\", width=3)\n",
    "    \n",
    "    # You can apply the mask to highlight the detected area\n",
    "    \n",
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e291e73-2795-4d95-9a72-7d7a9e6c8f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Generate bounding boxes for each object detected by Mask R-CNN in an image\n",
    "You can extract the bounding boxes from the Mask R-CNN output as shown:\n",
    "\n",
    "python\n",
    "Copy\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "# Load pre-trained Mask R-CNN model\n",
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Load the image\n",
    "image = Image.open(\"image.jpg\")\n",
    "image_tensor = torchvision.transforms.functional.to_tensor(image).unsqueeze(0)\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    predictions = model(image_tensor)\n",
    "\n",
    "# Get bounding boxes\n",
    "boxes = predictions[0]['boxes']\n",
    "\n",
    "# Draw bounding boxes\n",
    "draw = ImageDraw.Draw(image)\n",
    "for box in boxes:\n",
    "    draw.rectangle(box.tolist(), outline=\"red\", width=3)\n",
    "\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fedaecb-8e73-48e4-b7c5-89848efd199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#QUE 5. Convert an image to grayscale and apply Otsu's thresholding method for segmentation\n",
    "Here’s how you can use Otsu’s thresholding method:\n",
    "\n",
    "import cv2\n",
    "\n",
    "image = cv2.imread('dog.jpg')\n",
    "\n",
    "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "_, thresh = cv2.threshold(gray_image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "cv2.imshow(\"Otsu's Thresholding\", thresh)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc83e7d-5743-45ec-bf74-1eb7c46b978b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUE 6. Perform contour detection in an image to detect distinct objects or shapes\n",
    "\n",
    "import cv2\n",
    "\n",
    "image = cv2.imread('image.jpg')\n",
    "\n",
    "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "_, thresh = cv2.threshold(gray_image, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "cv2.drawContours(image, contours, -1, (0, 255, 0), 3)\n",
    "\n",
    "cv2.imshow(\"Contours\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2809ee68-39cc-4412-b400-66f56a4a3b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUE 7. Apply Mask R-CNN to detect objects and their segmentation masks in a custom image and display them\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "\n",
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "image = Image.open('image.jpg')\n",
    "image_tensor = torchvision.transforms.functional.to_tensor(image).unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = model(image_tensor)\n",
    "\n",
    "masks = predictions[0]['masks'] > 0.5  # Mask thresholding\n",
    "boxes = predictions[0]['boxes']\n",
    "\n",
    "draw = ImageDraw.Draw(image)\n",
    "for mask, box in zip(masks, boxes):\n",
    "    mask = mask[0].cpu().numpy()\n",
    "    draw.rectangle(box.tolist(), outline=\"red\", width=3)\n",
    "\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16725a3-7031-479a-bad9-5e9e2b12f1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUE 8  Apply k-means clustering for segmenting regions in an image\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('cat.jpg')\n",
    "\n",
    "pixels = image.reshape((-1, 3))\n",
    "\n",
    "pixels = np.float32(pixels)\n",
    "\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.2)\n",
    "k = 4  \n",
    "_, labels, centers = cv2.kmeans(pixels, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
    "\n",
    "centers = np.uint8(centers)\n",
    "\n",
    "segmented_image = centers[labels.flatten()]\n",
    "\n",
    "segmented_image = segmented_image.reshape(image.shape)\n",
    "\n",
    "cv2.imshow(\"K-means Segmentation\", segmented_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a03adc1-272f-498c-9bf1-8ffaec253992",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a70758-300f-4f97-a4d0-16b8d52478f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4824af01-f64b-4b5b-9ff9-9d2010cf7453",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e20ecf-6538-40f5-bcb8-65212242310b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d223fd-2058-4796-8f49-b0aff7709fd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
